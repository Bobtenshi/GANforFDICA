{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "ICA\n",
    "--------------------------------------------------------------------------\"\"\"\n",
    "def kurt(Y):\n",
    "    return torch.mean(Y**4, dim=0) - 3\n",
    "def kurt_loss(Y):\n",
    "    # Y の分散共分散が単位行列であることが前提=>白色化後なのでok\n",
    "    return -torch.mean(torch.abs(kurt(Y)))\n",
    "\n",
    "def ICA(X, loss_func = kurt_loss):\n",
    "    X = Variable(torch.from_numpy(X), requires_grad =False).float()\n",
    "    W = Variable((torch.randn((2, 2))), requires_grad =True)\n",
    "    W.data = W.data / torch.norm(W.data)\n",
    "    hist_loss = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        Y = torch.mm(X, W)\n",
    "        loss = loss_func(Y)\n",
    "        hist_loss.append(loss.data.numpy())\n",
    "        loss.backward()\n",
    "\n",
    "        W.data = W.data - W.grad.data * 0.1\n",
    "        W.grad.data.zero_()\n",
    "        W.data = W.data / torch.norm(W.data)\n",
    "    print(\"y_type=\"+str(type(Y)))\n",
    "    return Y, W, hist_loss\n",
    "\n",
    "#Y, W, hist_loss = ICA(X, loss_func=kurt_loss)\n",
    "#plt.plot(Y.data.numpy())\n",
    "#plt.show()\n",
    "#plt.plot(hist_loss)\n",
    "\n",
    "def FDICA(white_X1,white_X2, loss_func = kurt_loss):\n",
    "    \n",
    "    gyou = int(white_X1.shape[0])\n",
    "    retu = int(white_X1.shape[1])\n",
    "    \n",
    "    Y1 = np.zeros_like((white_X1), dtype='float')\n",
    "    Y2 = np.zeros_like((white_X1), dtype='float')\n",
    "    Wr = np.zeros([2,2,gyou],  dtype='float')\n",
    "    \n",
    "    Y1=torch.from_numpy(Y1)\n",
    "    Y2=torch.from_numpy(Y2)\n",
    "    Wr=torch.from_numpy(Wr)\n",
    "    \n",
    "    print(\"FDICA start\")\n",
    "    \n",
    "    \n",
    "    for f in range(gyou):\n",
    "        X1_temp = white_X1[ f ,:]\n",
    "        X2_temp = white_X1[ f ,:]\n",
    "        #X = [X1_temp,X2_temp]\n",
    "    \n",
    "       # X1_temp = X1_temp.T\n",
    "       # X2_temp = X2_temp.T\n",
    "        X = np.array((X1_temp,X2_temp), dtype='float')\n",
    "        #X = np.array((X1_temp,X2_temp))\n",
    "        #X = np.array(X)\n",
    "        X = X.T\n",
    "        \n",
    "        #print(\"X_fdica_type = \"+str(type(X)))\n",
    "        #print(\"X_fdica_size = \"+str(X.shape))\n",
    "        \n",
    "        X = Variable(torch.from_numpy(X), requires_grad =False).float()\n",
    "        W = Variable((torch.randn((2, 2))), requires_grad =True)\n",
    "        W.data = W.data / torch.norm(W.data)\n",
    "        hist_loss = []\n",
    "    \n",
    "        for i in range(10):\n",
    "            Y = torch.mm(X, W)\n",
    "            loss = loss_func(Y)\n",
    "            hist_loss.append(loss.data.numpy())\n",
    "            loss.backward()\n",
    "\n",
    "            W.data = W.data - W.grad.data * 0.1\n",
    "            W.grad.data.zero_()\n",
    "            W.data = W.data / torch.norm(W.data)\n",
    "        \n",
    "        Y.detach().numpy()\n",
    "        #print(\"Y_fdica_type = \"+str(type(Y)))\n",
    "        Y1[f, :] = Y[:,0]\n",
    "        Y2[f, :] = Y[:,1]\n",
    "        Wr[:,:,f] = W[:,:]\n",
    "    \n",
    "    Y1 = Y1.detach().numpy()\n",
    "    Y2 = Y2.detach().numpy()\n",
    "    print(\"FDICA finished\")\n",
    "    return Y1,Y2, Wr, hist_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
